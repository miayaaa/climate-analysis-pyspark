{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter session running on `mathmadslinux2p`.\n",
    "\n",
    "You can start your own Jupyter session on `mathmadslinux2p` and open this notebook in Chrome on the MADS Windows server by\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Login to the MADS Windows server using https://mathportal.canterbury.ac.nz/.\n",
    "2. Download or copy this notebook to your home directory.\n",
    "3. Open powershell and run `ssh mathmadslinux2p`.\n",
    "4. Run `start_pyspark_notebook` or `/opt/anaconda3/bin/jupyter-notebook --ip 132.181.129.68 --port $((8000 + $((RANDOM % 999))))`.\n",
    "5. Copy / paste the url provided in the shell window into Chrome on the MADS Windows server.\n",
    "6. Open the notebook from the Jupyter root directory (which is your home directory).\n",
    "7. Run `start_spark()` to start a spark session in the notebook.\n",
    "8. Run `stop_spark()` before closing the notebook or kill your spark application by hand using the link in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example notebook ###\n",
    "\n",
    "The code below provides a template for how you would use a notebook to start spark, run some code, and then stop spark.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Run `start_spark()` to start a spark session in the notebook (only change the default resources when advised to do so for an exercise or assignment)\n",
    "- Write and run code interactively, creating additional cells as needed.\n",
    "- Run `stop_spark()` before closing the notebook or kill your spark application by hand using the link in the [Spark UI](http://mathmadslinux2p.canterbury.ac.nz:8080/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>yya163 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4255\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20240915143817-1083</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1726367896736</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/yya163/DATA420_lab/Assignment_1/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/yya163/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>yya163 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4255</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>46731</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>yya163 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed modules\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The daily schema was defined in another notebook but is needed again here for processing\n",
    "\n",
    "daily_schema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True), \n",
    "    StructField(\"Element\", StringType(), True),\n",
    "    StructField(\"Value\", FloatType(), True),\n",
    "    StructField(\"MeasurementFlag\", StringType(), True),\n",
    "    StructField(\"QualityFlag\", StringType(), True),\n",
    "    StructField(\"SourceFlag\", StringType(), True),\n",
    "    StructField(\"ObservationTime\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Q1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   8 jsw93 jsw93  164424331 2024-08-07 01:40 /data/ghcnd/daily/2015.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  165799325 2024-08-07 01:42 /data/ghcnd/daily/2016.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  165395734 2024-08-07 01:43 /data/ghcnd/daily/2017.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  165492857 2024-08-07 01:41 /data/ghcnd/daily/2018.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  164296030 2024-08-07 01:40 /data/ghcnd/daily/2019.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  165204327 2024-08-07 01:43 /data/ghcnd/daily/2020.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  168079283 2024-08-07 01:38 /data/ghcnd/daily/2021.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  168959151 2024-08-07 01:42 /data/ghcnd/daily/2022.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93  168357302 2024-08-07 01:41 /data/ghcnd/daily/2023.csv.gz\n",
      "-rw-r--r--   8 jsw93 jsw93   88831735 2024-08-07 01:42 /data/ghcnd/daily/2024.csv.gz\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/ghcnd/daily/ | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey \"dfs.blocksize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=yya163&blocks=1&files=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2024.csv.gz\n",
      "FSCK started by yya163 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2024.csv.gz at Sun Sep 15 14:38:31 NZST 2024\n",
      "\n",
      "/data/ghcnd/daily/2024.csv.gz 88831735 bytes, replicated: replication=8, 1 block(s):  OK\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220563_479763 len=88831735 Live_repl=8\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t32\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t88831735 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 88831735 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t4\n",
      " Average block replication:\t8.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Sun Sep 15 14:38:31 NZST 2024 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/ghcnd/daily/2024.csv.gz' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /data/ghcnd/daily/2024.csv.gz -blocks -files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=yya163&blocks=1&files=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2023.csv.gz\n",
      "FSCK started by yya163 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2023.csv.gz at Sun Sep 15 14:38:33 NZST 2024\n",
      "\n",
      "/data/ghcnd/daily/2023.csv.gz 168357302 bytes, replicated: replication=8, 2 block(s):  OK\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220535_479735 len=134217728 Live_repl=8\n",
      "1. BP-700027894-132.181.129.68-1626517177804:blk_1074220536_479736 len=34139574 Live_repl=8\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t32\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t168357302 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t2 (avg. block size 84178651 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t4\n",
      " Average block replication:\t8.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Sun Sep 15 14:38:33 NZST 2024 in 0 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/ghcnd/daily/2023.csv.gz' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /data/ghcnd/daily/2023.csv.gz -blocks -files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=yya163&files=1&blocks=1&racks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2023.csv.gz\n",
      "FSCK started by yya163 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2023.csv.gz at Sun Sep 15 14:38:35 NZST 2024\n",
      "\n",
      "/data/ghcnd/daily/2023.csv.gz 168357302 bytes, replicated: replication=8, 2 block(s):  OK\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220535_479735 len=134217728 Live_repl=8  [/default-rack/192.168.40.159:9866, /default-rack/192.168.40.136:9866, /default-rack/192.168.40.182:9866, /default-rack/192.168.40.105:9866, /default-rack/192.168.40.145:9866, /default-rack/192.168.40.173:9866, /default-rack/192.168.40.106:9866, /default-rack/192.168.40.193:9866]\n",
      "1. BP-700027894-132.181.129.68-1626517177804:blk_1074220536_479736 len=34139574 Live_repl=8  [/default-rack/192.168.40.159:9866, /default-rack/192.168.40.158:9866, /default-rack/192.168.40.140:9866, /default-rack/192.168.40.102:9866, /default-rack/192.168.40.186:9866, /default-rack/192.168.40.180:9866, /default-rack/192.168.40.116:9866, /default-rack/192.168.40.111:9866]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t32\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t168357302 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t2 (avg. block size 84178651 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t4\n",
      " Average block replication:\t8.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Sun Sep 15 14:38:35 NZST 2024 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/ghcnd/daily/2023.csv.gz' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /data/ghcnd/daily/2023.csv.gz -files -blocks -racks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Q1(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Count the Number of Observations in 2023 and 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in 2023: 37867272\n",
      "Number of observations in 2024: 19720790\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"GHCN Analysis\").getOrCreate()\n",
    "\n",
    "daily_2023_df = spark.read.schema(daily_schema).csv(\"hdfs:///data/ghcnd/daily/2023.csv.gz\")\n",
    "daily_2024_df = spark.read.schema(daily_schema).csv(\"hdfs:///data/ghcnd/daily/2024.csv.gz\")\n",
    "\n",
    "# Count the number of observations for 2023 and 2024\n",
    "count_2023 = daily_2023_df.count()\n",
    "count_2024 = daily_2024_df.count()\n",
    "\n",
    "print(f\"Number of observations in 2023: {count_2023}\")\n",
    "print(f\"Number of observations in 2024: {count_2024}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for 2023 data: 1\n",
      "Number of partitions for 2024 data: 1\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions for 2023 and 2024 DataFrames\n",
    "num_partitions_2023 = daily_2023_df.rdd.getNumPartitions()\n",
    "num_partitions_2024 = daily_2024_df.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"Number of partitions for 2023 data: {num_partitions_2023}\")\n",
    "print(f\"Number of partitions for 2024 data: {num_partitions_2024}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations from 2014 to 2023: 370803270\n",
      "Number of partitions for multi-year data: 10\n"
     ]
    }
   ],
   "source": [
    "# Load the data from each range separately and then union them\n",
    "daily_multi_year_df = spark.read.schema(daily_schema).csv([\"hdfs:///data/ghcnd/daily/201[4-9].csv.gz\", \"hdfs:///data/ghcnd/daily/202[0-3].csv.gz\"])\n",
    "\n",
    "# Count the total number of observations\n",
    "total_count = daily_multi_year_df.count()\n",
    "print(f\"Total number of observations from 2014 to 2023: {total_count}\")\n",
    "\n",
    "\n",
    "# Get the number of partitions (which corresponds to the number of tasks)\n",
    "num_partitions_multi_year = daily_multi_year_df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions for multi-year data: {num_partitions_multi_year}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions (tasks) when loading all years in daily: 105\n"
     ]
    }
   ],
   "source": [
    "# Load all year data\n",
    "df_all_years = spark.read.csv(\"hdfs:///data/ghcnd/daily/*.csv.gz\")\n",
    " \n",
    "# Calculate the number of partitions \n",
    "num_partitions = df_all_years.rdd.getNumPartitions()\n",
    " \n",
    "print(f\"Number of partitions (tasks) when loading all years in daily: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>yya163 (jupyter)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
